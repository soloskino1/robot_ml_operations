{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6b48526",
   "metadata": {},
   "source": [
    "# Task 3: Q-learning for mobile robot navigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3499a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "### auxiliary functions\n",
    "\n",
    "def action_decode(act_code):\n",
    "    dirs = {0: \"N\", 1: \"E\", 2: \"S\", 3: \"W\"}\n",
    "    return dirs[act_code]\n",
    "\n",
    "def action_encode(act):\n",
    "    dir_codes = {\"N\": 0, \"E\": 1, \"S\": 2, \"W\": 3}\n",
    "    return dir_codes(act)\n",
    "\n",
    "def display_learning(series, label):\n",
    "    n_episodes = len(series)\n",
    "    show_n = 20\n",
    "    show_step = int(n_episodes/show_n)\n",
    "    sequence = []\n",
    "    for i in range(show_n):\n",
    "        sequence.append(np.mean(series[show_step*i:show_step*(i+1)]))\n",
    "        print((i+1) * show_step, ' episodes ', label, sequence[-1])\n",
    "    print('\\n')\n",
    "    plt.figure()\n",
    "    plt.plot(sequence)\n",
    "    plt.ylabel(label)\n",
    "    plt.xlabel('episodes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e581bb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class grid_env():\n",
    "### definition of the maze environment\n",
    "\n",
    "    def __init__(self, width = 5, height = 5, start = [0, 0], debug = False):\n",
    "        # Contructor methods create the environment with some given options\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.start = start\n",
    "        self.goal = [self.width - 1, self.height - 1]\n",
    "        self.debug = debug\n",
    "        self.n_states = self.width * self.height\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        # Reset method puts the state at the starting position\n",
    "        self.pos = self.start[:]   # columns, rows\n",
    "        return self.pos, 0, False        \n",
    "\n",
    "    def state_decode(self, obs_code):\n",
    "        r = obs_code // self.width\n",
    "        c = obs_code % self.width\n",
    "        return([c, r])\n",
    "    \n",
    "    def state_encode(self, position):\n",
    "        code = position[0] + position[1] * (self.width) # columns, rows\n",
    "        return(code)\n",
    "\n",
    "    def step(self, action):\n",
    "        # Depending on the action, update the environment state\n",
    "        if action == \"S\" and (self.pos[1] < self.height -1):\n",
    "            self.pos[1] += 1\n",
    "        elif action == \"N\" and self.pos[1] > 0:\n",
    "            self.pos[1] -= 1\n",
    "        elif action == \"W\" and self.pos[0] > 0:\n",
    "            self.pos[0] -= 1\n",
    "        elif action == \"E\" and (self.pos[0] < self.width -1):\n",
    "            self.pos[0] += 1\n",
    "\n",
    "        done = (self.pos == self.goal)  # check if goal was reached\n",
    "        if done:\n",
    "            reward = self.width + self.height  # reward at goal\n",
    "        else:\n",
    "            reward = -1  # negative reward at every step\n",
    "\n",
    "        if self.debug:\n",
    "            print(self.render())\n",
    "\n",
    "        return self.pos, reward, done\n",
    "\n",
    "    def render(self):\n",
    "        res = \"\"\n",
    "        for y in range(self.height):\n",
    "            for x in range(self.width):\n",
    "                if self.goal[0] == x and self.goal[1] == y:\n",
    "                    if self.pos[0] == x and self.pos[1] == y:\n",
    "                        res += \"@\"\n",
    "                    else:\n",
    "                        res += \"o\"\n",
    "                    continue\n",
    "                if self.pos[0] == x and self.pos[1] == y:\n",
    "                    res += \"x\"\n",
    "                else:\n",
    "                    res += \"_\"\n",
    "            res += \"\\n\"\n",
    "        return(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f386e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "class agent():\n",
    "### definition of the agent\n",
    "\n",
    "    def __init__(self, n_obs, discount = 1, learning_rate = 0.1, eps = {'start': 1, 'min': 0.01, 'decay': 0.001}):\n",
    "        self.action_space = np.asarray([0, 1, 2, 3])  # north, east, south, west\n",
    "        n_actions = np.shape(self.action_space)[0]\n",
    "        self.Q_table = np.zeros((n_obs, n_actions))\n",
    "\n",
    "        self.epsilon = eps['start']   #initialize the exploration probability to 1\n",
    "        self.epsilon_decay = eps['decay']   #exploration decreasing decay for exponential decreasing\n",
    "        self.epsilon_min = eps['min']   # minimum of exploration proba\n",
    "        \n",
    "        self.gamma = discount   #discounted factor\n",
    "        self.alpha = learning_rate   #learning rate\n",
    "    \n",
    "    def action_selection(self, state):\n",
    "        if np.random.uniform(0,1) < self.epsilon:\n",
    "            action = self.action_space[np.random.randint(0, 3)]   # choose a random action with probability epsilon\n",
    "        else:\n",
    "            action = np.argmax(self.Q_table[state,:])  # choose the best action for that state with prob 1-epsilon\n",
    "        return(action)\n",
    "\n",
    "    def policy_update(self, action, reward, state, next_state):\n",
    "        self.Q_table[state, action] = (1 - self.alpha) * self.Q_table[state, action] + self.alpha*(reward + self.gamma*max(self.Q_table[next_state,:]))\n",
    "\n",
    "    def decrease_exploration(self, e):\n",
    "        self.epsilon = max(self.epsilon_min, np.exp(-self.epsilon_decay*e))\n",
    "        \n",
    "    def test_agent(self, env):\n",
    "        state, _, done = env.reset()\n",
    "        steps = 0\n",
    "        while not done and steps < 100:\n",
    "            action = ag.action_selection(env.state_encode(state))\n",
    "            next_state, reward, done = env.step(action_decode(action))\n",
    "            steps += 1\n",
    "        print(steps)\n",
    "\n",
    "    def train(self, env, n_episodes = 1000, max_steps = 100):\n",
    "        all_rewards = []\n",
    "        all_steps = []\n",
    "        for e in range(n_episodes):   # iterate over episodes\n",
    "            state, _, done = env.reset()\n",
    "            trial_reward = 0\n",
    "            t = 0\n",
    "            while not done and t < max_steps:\n",
    "                action = ag.action_selection(env.state_encode(state))  # step 1: choose an action\n",
    "                old_state = state[:]\n",
    "                next_state, reward, done = env.step(action_decode(action))    # steps 2 and 3: The environment runs the chosen action and returns next state and reward\n",
    "                ag.policy_update(action, reward, env.state_encode(old_state), env.state_encode(next_state))  # step 4: policy update\n",
    "                trial_reward += reward\n",
    "                t += 1\n",
    "            ag.decrease_exploration(e)\n",
    "            all_rewards.append(trial_reward)\n",
    "            all_steps.append(t)\n",
    "        return(all_rewards, all_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c7a922",
   "metadata": {},
   "source": [
    "Task 3\n",
    "\n",
    "A.\tModify the maze, changing size and shape, and different start positions. How many steps does it take to reach the target? Does the performance vary as you expect?\n",
    "\n",
    "B.\tChange the agent learning parameters (e.g. learning rate, discount factor, exploration values). How does performance change in terms of learning speed and ability to reach the target? What happens if exploration is always maximum? And if it decreases very quickly?\n",
    "\n",
    "C. Optional. Change the reward applied to different types of actions and test the learning performance. Are you able to find values for which learning is even faster? Imagine that there was a hole in the maze: how can you make the agent learn to avoid it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff8309b",
   "metadata": {},
   "source": [
    "# Your submission below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b5ffbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this code is only an example, remove it or change it\n",
    "\n",
    "maze_height = 10\n",
    "maze_width = 10\n",
    "start = [0, 0]\n",
    "maze = grid_env(maze_height, maze_width, start)\n",
    "\n",
    "epsilon = {'start': 1, 'min': 0.01, 'decay': 0.001} # parameter epsilon needs to be a dictionary\n",
    "ag = agent(maze.n_states, eps = epsilon) # only one parameter is compulsory, try adding different values of discount factor and learing rate\n",
    "episodes = 5000\n",
    "steps = 100\n",
    "[rewards, steps] = ag.train(maze, episodes, steps)\n",
    "\n",
    "display_learning(rewards, \"reward \")\n",
    "display_learning(steps, \"steps \")\n",
    "\n",
    "print(ag.Q_table)\n",
    "\n",
    "maze = grid_env(maze_height, maze_width, start, debug = True)\n",
    "ag.test_agent(maze)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4d89b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
